{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Steps to complete\\n1. Add the bucketization logic to the notebook\\n2. Add the cross-variable logic to the notebook\\n3. Add JSON input parameter and decode into dict\\n4. Combine cells into .py script\\n5. ingest train and test files into Splice tables\\n6. add new column for the label to the tables\\n7. add the saver-def code to save off the model\\n7. write a stored procedure that exports the tables to two files, creates a JSON map that will be,\\ninput to the python script, call the script from the stored procedure, train the model, save it\"\\n8. write a new stored procedure that deploys the model\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Steps to complete\n",
    "1. Add the bucketization logic to the notebook\n",
    "2. Add the cross-variable logic to the notebook\n",
    "3. Add JSON input parameter and decode into dict\n",
    "4. Combine cells into .py script\n",
    "5. ingest train and test files into Splice tables\n",
    "6. add new column for the label to the tables\n",
    "7. add the saver-def code to save off the model\n",
    "7. write a stored procedure that exports the tables to two files, creates a JSON map that will be,\n",
    "input to the python script, call the script from the stored procedure, train the model, save it\"\n",
    "8. write a new stored procedure that deploys the model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Example code for TensorFlow Wide & Deep Tutorial using TF.Learn API.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import tempfile\n",
    "from six.moves import urllib\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will error if you run more than once\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string(\"model_dir\", \"\", \"Base directory for output models.\")\n",
    "flags.DEFINE_string(\"model_type\", \"wide_n_deep\",\n",
    "                    \"Valid model types: {'wide', 'deep', 'wide_n_deep'}.\")\n",
    "flags.DEFINE_integer(\"train_steps\", 200, \"Number of training steps.\")\n",
    "flags.DEFINE_string(\n",
    "    \"train_data\",\n",
    "    \"\",\n",
    "    \"Path to the training data.\")\n",
    "flags.DEFINE_string(\n",
    "    \"test_data\",\n",
    "    \"\",\n",
    "    \"Path to the test data.\")\n",
    "flags.DEFINE_string(\n",
    "    \"inputs\",\n",
    "    '{\"columns\": [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education_num\",\"marital_status\",\"occupation\",\"relationship\",\"race\",\"gender\",\"capital_gain\",\"capital_loss\",\"hours_per_week\",\"native_country\",\"income_bracket\"],\"categorical_columns\": [\"workclass\",\"education\",\"marital_status\",\"occupation\",\"relationship\",\"race\",\"gender\",\"native_country\"],\"continuous_columns\": [\"age\",\"education_num\",\"capital_gain\",\"capital_loss\",\"hours_per_week\"],\"label_column\": \"label\",\"bucketized_columns\": {\"age_buckets\": {      \"age\": [    18,    25,    30,    35,    40,    45,    50,    55,    60,    65  ]}},\"crossed_columns\": [[  \"education\",  \"occupation\"],[  \"age_buckets\",  \"education\",  \"occupation\"],[  \"native_country\",  \"occupation\"]],\"train_data_path\": \"/Users/erindriggers/anaconda/envs/tensorflow/projects/wide_n_deep/data/train/part-r-00000.csv\",\"test_data_path\": \"/Users/erindriggers/anaconda/envs/tensorflow/projects/wide_n_deep/data/test/part-r-00000.csv\"}',\n",
    "    \"Input data dictionary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DICT={'test_data_path': '/Users/erindriggers/anaconda/envs/tensorflow/projects/wide_n_deep/data/test/part-r-00000.csv', 'categorical_columns': ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'native_country'], 'bucketized_columns': {'age_buckets': {'age': [18, 25, 30, 35, 40, 45, 50, 55, 60, 65]}}, 'label_column': 'label', 'crossed_columns': [['education', 'occupation'], ['age_buckets', 'education', 'occupation'], ['native_country', 'occupation']], 'columns': ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income_bracket'], 'train_data_path': '/Users/erindriggers/anaconda/envs/tensorflow/projects/wide_n_deep/data/train/part-r-00000.csv', 'continuous_columns': ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']}\n",
      "COLUMNS=['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income_bracket']\n",
      "CATEGORICAL_COLUMNS=['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'native_country']\n",
      "CONTINUOUS_COLUMNS=['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
      "LABEL_COLUMN=label\n",
      "COLUMNS=['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income_bracket']\n",
      "BUCKETIZED_COLUMNS={'age_buckets': {'age': [18, 25, 30, 35, 40, 45, 50, 55, 60, 65]}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## TBD: The dict below should be input to teh script and constructed by a stored procedure\n",
    "## The Stored Procedure can construct JSON and then this script can decode the JSON into the dict\n",
    "## The paths to the files below should be paths constructed by a Splice Machine Export\n",
    "\n",
    "\n",
    "INPUT_DICT=json.loads(FLAGS.inputs)\n",
    "COLUMNS = INPUT_DICT['columns'];\n",
    "LABEL_COLUMN = INPUT_DICT['label_column'];\n",
    "CATEGORICAL_COLUMNS = INPUT_DICT['categorical_columns'];\n",
    "CONTINUOUS_COLUMNS = INPUT_DICT['continuous_columns'];\n",
    "CROSSED_COLUMNS = INPUT_DICT['crossed_columns'];\n",
    "BUCKETIZED_COLUMNS = INPUT_DICT['bucketized_columns'];\n",
    "\n",
    "FLAGS.train_data=INPUT_DICT['train_data_path'];\n",
    "FLAGS.test_data=INPUT_DICT['test_data_path'];\n",
    "\n",
    "print(\"INPUT_DICT=%s\" % INPUT_DICT)\n",
    "print(\"COLUMNS=%s\" % COLUMNS)\n",
    "print(\"CATEGORICAL_COLUMNS=%s\" % CATEGORICAL_COLUMNS)\n",
    "print(\"CONTINUOUS_COLUMNS=%s\" % CONTINUOUS_COLUMNS)\n",
    "print(\"LABEL_COLUMN=%s\" % LABEL_COLUMN)\n",
    "print(\"COLUMNS=%s\" % COLUMNS)\n",
    "print(\"BUCKETIZED_COLUMNS=%s\" % BUCKETIZED_COLUMNS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def maybe_download():\n",
    "  \"\"\"May be downloads training data and returns train and test file names.\"\"\"\n",
    "  if FLAGS.train_data:\n",
    "    train_file_name = FLAGS.train_data\n",
    "  else:\n",
    "    train_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    urllib.request.urlretrieve(INPUT_DICT['train_data_path'], train_file.name)  # pylint: disable=line-too-long\n",
    "    train_file_name = train_file.name\n",
    "    train_file.close()\n",
    "    print(\"Training data is downloaded to %s\" % train_file_name)\n",
    "\n",
    "  if FLAGS.test_data:\n",
    "    test_file_name = FLAGS.test_data\n",
    "  else:\n",
    "    test_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    urllib.request.urlretrieve(INPUT_DICT['test_data_path'], test_file.name)  # pylint: disable=line-too-long\n",
    "    test_file_name = test_file.name\n",
    "    test_file.close()\n",
    "    print(\"Test data is downloaded to %s\" % test_file_name)\n",
    "\n",
    "  return train_file_name, test_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sparse_columns(cols):\n",
    "    \"\"\"Creates tf sparse columns with hash buckets\"\"\"\n",
    "    # Sparse base columns.\n",
    "    # TBD: allow keyed columns and hash bucket size as input\n",
    "    tf_cols ={}\n",
    "    for col in cols :\n",
    "        tf_cols[col] = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "          col, hash_bucket_size=1000)\n",
    "    return tf_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relationship': _SparseColumnHashed(column_name='relationship', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'native_country': _SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'gender': _SparseColumnHashed(column_name='gender', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'education': _SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'workclass': _SparseColumnHashed(column_name='workclass', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'occupation': _SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'marital_status': _SparseColumnHashed(column_name='marital_status', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'race': _SparseColumnHashed(column_name='race', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)}\n"
     ]
    }
   ],
   "source": [
    "SPARSE_TF_COLUMNS = prepare_sparse_columns(CATEGORICAL_COLUMNS)\n",
    "print(SPARSE_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_continuous_columns(cols):\n",
    "    \"\"\"Creates tf.contrib.layers.real_valued_columns\"\"\"\n",
    "    #Continuous base columns\n",
    "    tf_cols ={}\n",
    "    for col in cols :\n",
    "        tf_cols[col] = (tf.contrib.layers.real_valued_column(col))\n",
    "    return tf_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capital_gain': _RealValuedColumn(column_name='capital_gain', dimension=1, default_value=None, dtype=tf.float32), 'hours_per_week': _RealValuedColumn(column_name='hours_per_week', dimension=1, default_value=None, dtype=tf.float32), 'age': _RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32), 'capital_loss': _RealValuedColumn(column_name='capital_loss', dimension=1, default_value=None, dtype=tf.float32), 'education_num': _RealValuedColumn(column_name='education_num', dimension=1, default_value=None, dtype=tf.float32)}\n"
     ]
    }
   ],
   "source": [
    "REAL_TF_COLUMNS = prepare_continuous_columns(CONTINUOUS_COLUMNS)\n",
    "print(REAL_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_buckets(cols):\n",
    "    \"\"\"Creates tf bucketed columns\"\"\"\n",
    "    new_cols = {}\n",
    "    for newCol, tuple in cols.items():\n",
    "        orig_col = REAL_TF_COLUMNS[tuple[0]]\n",
    "    new_cols[newCol] = tf.contrib.layers.bucketized_column(orig_col, boundaries=tuple[1])\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age_buckets': {'age': [18, 25, 30, 35, 40, 45, 50, 55, 60, 65]}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-55a3eb7fa775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUCKETIZED_COLUMNS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mBUCKETIZED_TF_COLUMNS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_buckets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUCKETIZED_COLUMNS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-f52b8651fc9b>\u001b[0m in \u001b[0;36mprepare_buckets\u001b[0;34m(cols)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnew_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnewCol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0morig_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREAL_TF_COLUMNS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnew_cols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnewCol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucketized_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "print(BUCKETIZED_COLUMNS)\n",
    "BUCKETIZED_TF_COLUMNS = prepare_buckets(BUCKETIZED_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_embedded_columns(cols):\n",
    "    \"\"\"Create tf.contrib.layers.embedding_columns for the sparse entries\"\"\"\n",
    "    tf_cols = {}\n",
    "    for col in cols:\n",
    "        tf_cols[col] = tf.contrib.layers.embedding_column(col, dimension=8)\n",
    "    return tf_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['relationship', 'native_country', 'gender', 'education', 'workclass', 'occupation', 'marital_status', 'race']\n"
     ]
    }
   ],
   "source": [
    "print(list(SPARSE_TF_COLUMNS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EMBEDDED_TF_COLUMNS = prepare_embedded_columns(list(SPARSE_TF_COLUMNS.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{_SparseColumnHashed(column_name='gender', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='gender', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x120245620>), _SparseColumnHashed(column_name='workclass', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='workclass', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x120245730>), _SparseColumnHashed(column_name='marital_status', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='marital_status', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x120239f28>), _SparseColumnHashed(column_name='relationship', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='relationship', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x1202451e0>), _SparseColumnHashed(column_name='race', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='race', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x120239ea0>), _SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x1202456a8>), _SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x120245268>), _SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x1202457b8>)}\n"
     ]
    }
   ],
   "source": [
    "print(EMBEDDED_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='gender', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x120245620>), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='workclass', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x120245730>), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='marital_status', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x120239f28>), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='relationship', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x1202451e0>), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='race', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x120239ea0>), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x1202456a8>), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x120245268>), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x1202457b8>), _RealValuedColumn(column_name='capital_gain', dimension=1, default_value=None, dtype=tf.float32), _RealValuedColumn(column_name='hours_per_week', dimension=1, default_value=None, dtype=tf.float32), _RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32), _RealValuedColumn(column_name='capital_loss', dimension=1, default_value=None, dtype=tf.float32), _RealValuedColumn(column_name='education_num', dimension=1, default_value=None, dtype=tf.float32)]\n"
     ]
    }
   ],
   "source": [
    "DEEP_TF_COLUMNS =  list(EMBEDDED_TF_COLUMNS.values()) + list(REAL_TF_COLUMNS.values())\n",
    "print(DEEP_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BUCKETIZED_TF_COLUMNS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6147ff2551f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mWIDE_TF_COLUMNS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPARSE_TF_COLUMNS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUCKETIZED_TF_COLUMNS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWIDE_TF_COLUMNS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BUCKETIZED_TF_COLUMNS' is not defined"
     ]
    }
   ],
   "source": [
    "WIDE_TF_COLUMNS = list(SPARSE_TF_COLUMNS.values()) + list(BUCKETIZED_TF_COLUMNS.values())\n",
    "print(WIDE_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_estimator(model_dir):\n",
    "  \"\"\"Build an estimator.\"\"\"\n",
    "  m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "    model_dir=model_dir,\n",
    "    linear_feature_columns=WIDE_TF_COLUMNS,\n",
    "    dnn_feature_columns=DEEP_TF_COLUMNS,\n",
    "    dnn_hidden_units=[100, 50])\n",
    "  return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(df):\n",
    "  \"\"\"Input builder function.\"\"\"\n",
    "  # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "  # the values of that column stored in a constant Tensor.\n",
    "  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n",
    "  # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "  # to the values of that column stored in a tf.SparseTensor.\n",
    "  categorical_cols = {k: tf.SparseTensor(\n",
    "      indices=[[i, 0] for i in range(df[k].size)],\n",
    "      values=df[k].values,\n",
    "      shape=[df[k].size, 1])\n",
    "                      for k in CATEGORICAL_COLUMNS}\n",
    "  # Merges the two dictionaries into one.\n",
    "  feature_cols = dict(continuous_cols)\n",
    "  feature_cols.update(categorical_cols)\n",
    "  # Converts the label column into a constant Tensor.\n",
    "  label = tf.constant(df[LABEL_COLUMN].values)\n",
    "  # Returns the feature columns and the label.\n",
    "  return feature_cols, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval():\n",
    "  \"\"\"Train and evaluate the model.\"\"\"\n",
    "  train_file_name, test_file_name = maybe_download()\n",
    "  df_train = pd.read_csv(\n",
    "      tf.gfile.Open(train_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      engine=\"python\")\n",
    "  df_test = pd.read_csv(\n",
    "      tf.gfile.Open(test_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      skiprows=1,\n",
    "      engine=\"python\")\n",
    "    \n",
    "# Temp hack - label should come in the input \n",
    "  df_train[LABEL_COLUMN] = (\n",
    "      df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "  df_test[LABEL_COLUMN] = (\n",
    "      df_test[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "\n",
    "  model_dir = tempfile.mkdtemp() if not FLAGS.model_dir else FLAGS.model_dir\n",
    "  print(\"model directory = %s\" % model_dir)\n",
    "\n",
    "  m = build_estimator(model_dir)\n",
    "  m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\n",
    "  results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)\n",
    "  for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_and_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
