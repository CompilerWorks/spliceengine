{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Steps to complete\\n1. Add the bucketization logic to the notebook\\n2. Add the cross-variable logic to the notebook\\n3. Add JSON input parameter and decode into dict\\n4. Combine cells into .py script\\n5. ingest train and test files into Splice tables\\n6. add new column for the label to the tables\\n7. add the saver-def code to save off the model\\n7. write a stored procedure that exports the tables to two files, creates a JSON map that will be,\\ninput to the python script, call the script from the stored procedure, train the model, save it\"\\n8. write a new stored procedure that deploys the model\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Steps to complete\n",
    "1. Add the bucketization logic to the notebook\n",
    "2. Add the cross-variable logic to the notebook\n",
    "3. Add JSON input parameter and decode into dict\n",
    "4. Combine cells into .py script\n",
    "5. ingest train and test files into Splice tables\n",
    "6. add new column for the label to the tables\n",
    "7. add the saver-def code to save off the model\n",
    "7. write a stored procedure that exports the tables to two files, creates a JSON map that will be,\n",
    "input to the python script, call the script from the stored procedure, train the model, save it\"\n",
    "8. write a new stored procedure that deploys the model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Example code for TensorFlow Wide & Deep Tutorial using TF.Learn API.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tempfile\n",
    "from six.moves import urllib\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will error if you run more than once\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string(\"model_dir\", \"\", \"Base directory for output models.\")\n",
    "flags.DEFINE_string(\"model_type\", \"wide_n_deep\",\n",
    "                    \"Valid model types: {'wide', 'deep', 'wide_n_deep'}.\")\n",
    "flags.DEFINE_integer(\"train_steps\", 200, \"Number of training steps.\")\n",
    "flags.DEFINE_string(\n",
    "    \"train_data\",\n",
    "    \"\",\n",
    "    \"Path to the training data.\")\n",
    "flags.DEFINE_string(\n",
    "    \"test_data\",\n",
    "    \"\",\n",
    "    \"Path to the test data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DICT={'crossed_columns': [['education', 'occupation'], ['age_bucketseducation', 'occupation'], ['native_country', 'occupation']], 'train_data_path': 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', 'label_column': 'label', 'columns': ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income_bracket'], 'bucketized_columns': {'age_buckets': ('age', [18, 25, 30, 35, 40, 45, 50, 55, 60, 65])}, 'categorical_columns': ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'native_country'], 'continuous_columns': ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week'], 'test_data_path': 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'}\n",
      "COLUMNS=['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income_bracket']\n",
      "CATEGORICAL_COLUMNS=['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'native_country']\n",
      "CONTINUOUS_COLUMNS=['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
      "LABEL_COLUMN=label\n",
      "COLUMNS=['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income_bracket']\n",
      "BUCKETIZED_COLUMNS={'age_buckets': ('age', [18, 25, 30, 35, 40, 45, 50, 55, 60, 65])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## TBD: The dict below should be input to teh script and constructed by a stored procedure\n",
    "## The Stored Procedure can construct JSON and then this script can decode the JSON into the dict\n",
    "## The paths to the files below should be paths constructed by a Splice Machine Export\n",
    "\n",
    "\n",
    "INPUT_DICT = { 'columns' : ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "      'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "      'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "      'native_country','income_bracket'],\n",
    "    'categorical_columns': ['workclass', 'education', 'marital_status', 'occupation',\n",
    "                            'relationship', 'race', 'gender', 'native_country'],\n",
    "    'continuous_columns' : ['age', 'education_num', 'capital_gain', 'capital_loss','hours_per_week'],\n",
    "    'label_column' : 'label',\n",
    "    'bucketized_columns' : dict({'age_buckets' : ('age', [18, 25, 30, 35, 40, 45, 50, 55, 60, 65])}),\n",
    "    'crossed_columns' : [ ['education', 'occupation' ], ['age_buckets' 'education', 'occupation'],\n",
    "                         ['native_country', 'occupation'] ],\n",
    "    'train_data_path' : \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "    'test_data_path' :  \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    "    }\n",
    "COLUMNS = INPUT_DICT['columns'];\n",
    "LABEL_COLUMN = INPUT_DICT['label_column'];\n",
    "CATEGORICAL_COLUMNS = INPUT_DICT['categorical_columns'];\n",
    "CONTINUOUS_COLUMNS = INPUT_DICT['continuous_columns'];\n",
    "CROSSED_COLUMNS = INPUT_DICT['crossed_columns'];\n",
    "BUCKETIZED_COLUMNS = INPUT_DICT['bucketized_columns'];\n",
    "\n",
    "print(\"INPUT_DICT=%s\" % INPUT_DICT)\n",
    "print(\"COLUMNS=%s\" % COLUMNS)\n",
    "print(\"CATEGORICAL_COLUMNS=%s\" % CATEGORICAL_COLUMNS)\n",
    "print(\"CONTINUOUS_COLUMNS=%s\" % CONTINUOUS_COLUMNS)\n",
    "print(\"LABEL_COLUMN=%s\" % LABEL_COLUMN)\n",
    "print(\"COLUMNS=%s\" % COLUMNS)\n",
    "print(\"BUCKETIZED_COLUMNS=%s\" % BUCKETIZED_COLUMNS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def maybe_download():\n",
    "  \"\"\"May be downloads training data and returns train and test file names.\"\"\"\n",
    "  if FLAGS.train_data:\n",
    "    train_file_name = FLAGS.train_data\n",
    "  else:\n",
    "    train_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    urllib.request.urlretrieve(INPUT_DICT['train_data_path'], train_file.name)  # pylint: disable=line-too-long\n",
    "    train_file_name = train_file.name\n",
    "    train_file.close()\n",
    "    print(\"Training data is downloaded to %s\" % train_file_name)\n",
    "\n",
    "  if FLAGS.test_data:\n",
    "    test_file_name = FLAGS.test_data\n",
    "  else:\n",
    "    test_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    urllib.request.urlretrieve(INPUT_DICT['test_data_path'], test_file.name)  # pylint: disable=line-too-long\n",
    "    test_file_name = test_file.name\n",
    "    test_file.close()\n",
    "    print(\"Test data is downloaded to %s\" % test_file_name)\n",
    "\n",
    "  return train_file_name, test_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sparse_columns(cols):\n",
    "    \"\"\"Creates tf sparse columns with hash buckets\"\"\"\n",
    "    # Sparse base columns.\n",
    "    # TBD: allow keyed columns and hash bucket size as input\n",
    "    tf_cols ={}\n",
    "    for col in cols :\n",
    "        tf_cols[col] = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "          col, hash_bucket_size=1000)\n",
    "    return tf_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'marital_status': _SparseColumnHashed(column_name='marital_status', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'occupation': _SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'education': _SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'workclass': _SparseColumnHashed(column_name='workclass', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'gender': _SparseColumnHashed(column_name='gender', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'native_country': _SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'race': _SparseColumnHashed(column_name='race', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'relationship': _SparseColumnHashed(column_name='relationship', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)}\n"
     ]
    }
   ],
   "source": [
    "SPARSE_TF_COLUMNS = prepare_sparse_columns(CATEGORICAL_COLUMNS)\n",
    "print(SPARSE_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_continuous_columns(cols):\n",
    "    \"\"\"Creates tf.contrib.layers.real_valued_columns\"\"\"\n",
    "    #Continuous base columns\n",
    "    tf_cols ={}\n",
    "    for col in cols :\n",
    "        tf_cols[col] = (tf.contrib.layers.real_valued_column(col))\n",
    "    return tf_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capital_loss': _RealValuedColumn(column_name='capital_loss', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), 'education_num': _RealValuedColumn(column_name='education_num', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), 'hours_per_week': _RealValuedColumn(column_name='hours_per_week', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), 'age': _RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), 'capital_gain': _RealValuedColumn(column_name='capital_gain', dimension=1, default_value=None, dtype=tf.float32, normalizer=None)}\n"
     ]
    }
   ],
   "source": [
    "REAL_TF_COLUMNS = prepare_continuous_columns(CONTINUOUS_COLUMNS)\n",
    "print(REAL_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_buckets(cols):\n",
    "    \"\"\"Creates tf bucketed columns\"\"\"\n",
    "    new_cols = {}\n",
    "    for newCol, tuple in cols.items():\n",
    "        orig_col = REAL_TF_COLUMNS[tuple[0]]\n",
    "    new_cols[newCol] = tf.contrib.layers.bucketized_column(orig_col, boundaries=tuple[1])\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age_buckets': ('age', [18, 25, 30, 35, 40, 45, 50, 55, 60, 65])}\n"
     ]
    }
   ],
   "source": [
    "print(BUCKETIZED_COLUMNS)\n",
    "BUCKETIZED_TF_COLUMNS = prepare_buckets(BUCKETIZED_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_embedded_columns(cols):\n",
    "    \"\"\"Create tf.contrib.layers.embedding_columns for the sparse entries\"\"\"\n",
    "    tf_cols = {}\n",
    "    for col in cols:\n",
    "        tf_cols[col] = tf.contrib.layers.embedding_column(col, dimension=8)\n",
    "    return tf_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marital_status', 'occupation', 'education', 'workclass', 'gender', 'native_country', 'race', 'relationship']\n"
     ]
    }
   ],
   "source": [
    "print(list(SPARSE_TF_COLUMNS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    }
   ],
   "source": [
    "EMBEDDED_TF_COLUMNS = prepare_embedded_columns(list(SPARSE_TF_COLUMNS.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{_SparseColumnHashed(column_name='marital_status', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='marital_status', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x1163689d8>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None), _SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x11637cd90>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None), _SparseColumnHashed(column_name='workclass', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='workclass', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x116368e18>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None), _SparseColumnHashed(column_name='gender', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='gender', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x11637c510>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None), _SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x116368ae8>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None), _SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x116368d08>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None), _SparseColumnHashed(column_name='race', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='race', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x11637c950>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None), _SparseColumnHashed(column_name='relationship', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='relationship', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x11637c268>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None)}\n"
     ]
    }
   ],
   "source": [
    "print(EMBEDDED_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='marital_status', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x1163689d8>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x11637cd90>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='workclass', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x116368e18>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='gender', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x11637c510>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x116368ae8>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x116368d08>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='race', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x11637c950>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='relationship', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x11637c268>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None), _RealValuedColumn(column_name='capital_loss', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='education_num', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='hours_per_week', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='capital_gain', dimension=1, default_value=None, dtype=tf.float32, normalizer=None)]\n"
     ]
    }
   ],
   "source": [
    "DEEP_TF_COLUMNS =  list(EMBEDDED_TF_COLUMNS.values()) + list(REAL_TF_COLUMNS.values())\n",
    "print(DEEP_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_SparseColumnHashed(column_name='marital_status', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='workclass', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='gender', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='race', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='relationship', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _BucketizedColumn(source_column=_RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65))]\n"
     ]
    }
   ],
   "source": [
    "WIDE_TF_COLUMNS = list(SPARSE_TF_COLUMNS.values()) + list(BUCKETIZED_TF_COLUMNS.values())\n",
    "print(WIDE_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_estimator(model_dir):\n",
    "  \"\"\"Build an estimator.\"\"\"\n",
    "  m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "    model_dir=model_dir,\n",
    "    linear_feature_columns=WIDE_TF_COLUMNS,\n",
    "    dnn_feature_columns=DEEP_TF_COLUMNS,\n",
    "    dnn_hidden_units=[100, 50])\n",
    "  return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(df):\n",
    "  \"\"\"Input builder function.\"\"\"\n",
    "  # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "  # the values of that column stored in a constant Tensor.\n",
    "  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n",
    "  # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "  # to the values of that column stored in a tf.SparseTensor.\n",
    "  categorical_cols = {k: tf.SparseTensor(\n",
    "      indices=[[i, 0] for i in range(df[k].size)],\n",
    "      values=df[k].values,\n",
    "      shape=[df[k].size, 1])\n",
    "                      for k in CATEGORICAL_COLUMNS}\n",
    "  # Merges the two dictionaries into one.\n",
    "  feature_cols = dict(continuous_cols)\n",
    "  feature_cols.update(categorical_cols)\n",
    "  # Converts the label column into a constant Tensor.\n",
    "  label = tf.constant(df[LABEL_COLUMN].values)\n",
    "  # Returns the feature columns and the label.\n",
    "  return feature_cols, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval():\n",
    "  \"\"\"Train and evaluate the model.\"\"\"\n",
    "  train_file_name, test_file_name = maybe_download()\n",
    "  df_train = pd.read_csv(\n",
    "      tf.gfile.Open(train_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      engine=\"python\")\n",
    "  df_test = pd.read_csv(\n",
    "      tf.gfile.Open(test_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      skiprows=1,\n",
    "      engine=\"python\")\n",
    "    \n",
    "# Temp hack - label should come in the input \n",
    "  df_train[LABEL_COLUMN] = (\n",
    "      df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "  df_test[LABEL_COLUMN] = (\n",
    "      df_test[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "\n",
    "  model_dir = tempfile.mkdtemp() if not FLAGS.model_dir else FLAGS.model_dir\n",
    "  print(\"model directory = %s\" % model_dir)\n",
    "\n",
    "  m = build_estimator(model_dir)\n",
    "  m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\n",
    "  results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)\n",
    "  for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is downloaded to /var/folders/c_/01b20ch176d7nh4zw1025w580000gr/T/tmpfn3plfsk\n",
      "Test data is downloaded to /var/folders/c_/01b20ch176d7nh4zw1025w580000gr/T/tmpokmkbomp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Change warning: default value of `enable_centered_bias` will change after 2016-10-09. It will be disabled by default.Instructions for keeping existing behaviour:\n",
      "Explicitly set `enable_centered_bias` to 'True' if you want to keep existing behaviour.\n",
      "WARNING:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model directory = /var/folders/c_/01b20ch176d7nh4zw1025w580000gr/T/tmpq59m3xpj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Given features: {'age': <tf.Tensor 'Const:0' shape=(16281,) dtype=int64>, 'capital_gain': <tf.Tensor 'Const_2:0' shape=(16281,) dtype=int64>, 'occupation': <tensorflow.python.framework.ops.SparseTensor object at 0x1163aeb38>, 'native_country': <tensorflow.python.framework.ops.SparseTensor object at 0x1223b9fd0>, 'gender': <tensorflow.python.framework.ops.SparseTensor object at 0x11d641160>, 'marital_status': <tensorflow.python.framework.ops.SparseTensor object at 0x1200b44a8>, 'capital_loss': <tf.Tensor 'Const_3:0' shape=(16281,) dtype=int64>, 'education_num': <tf.Tensor 'Const_1:0' shape=(16281,) dtype=int64>, 'hours_per_week': <tf.Tensor 'Const_4:0' shape=(16281,) dtype=int64>, 'education': <tensorflow.python.framework.ops.SparseTensor object at 0x11d46b828>, 'relationship': <tensorflow.python.framework.ops.SparseTensor object at 0x11c43cf28>, 'race': <tensorflow.python.framework.ops.SparseTensor object at 0x12382cf98>, 'workclass': <tensorflow.python.framework.ops.SparseTensor object at 0x11ad6f0b8>}, required signatures: {'workclass': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'age': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(32561)]), is_sparse=False), 'gender': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'capital_loss': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(32561)]), is_sparse=False), 'native_country': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'capital_gain': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(32561)]), is_sparse=False), 'marital_status': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'occupation': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'education_num': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(32561)]), is_sparse=False), 'hours_per_week': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(32561)]), is_sparse=False), 'education': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'race': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'relationship': TensorSignature(dtype=tf.string, shape=None, is_sparse=True)}.\n",
      "WARNING:tensorflow:Given targets: Tensor(\"Const_5:0\", shape=(16281,), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(32561)]), is_sparse=False).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.842884\n",
      "accuracy/baseline_target_mean: 0.236226\n",
      "accuracy/threshold_0.500000_mean: 0.842884\n",
      "auc: 0.882235\n",
      "global_step: 200\n",
      "labels/actual_target_mean: 0.236226\n",
      "labels/prediction_mean: 0.214969\n",
      "loss: 0.422239\n",
      "precision/positive_threshold_0.500000_mean: 0.747312\n",
      "recall/positive_threshold_0.500000_mean: 0.50598\n"
     ]
    }
   ],
   "source": [
    "train_and_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module '__main__' has no attribute 'main'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-44cda3e31e6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main)\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mflags_passthrough\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags_passthrough\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module '__main__' has no attribute 'main'"
     ]
    }
   ],
   "source": [
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
